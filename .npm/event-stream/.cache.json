{"_id":"event-stream","_rev":"142-4f370711bbcffd447ca1100c0bdcabb4","name":"event-stream","description":"construct pipes of streams of events","dist-tags":{"latest":"3.0.16"},"versions":{"0.1.0":{"name":"event-stream","version":"0.1.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{},"devDependencies":{},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.1.0/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.1.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.13","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"6e9d2d059cf66deeb001d211bd60e45a9c0ae05e","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.1.0.tgz"},"scripts":{},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.2.0":{"name":"event-stream","version":"0.2.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{},"devDependencies":{},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.2.0/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.2.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.27","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"1cbcb8c7b361e5c7d2b72738afca5b6fdfcd31af","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.2.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.2.1":{"name":"event-stream","version":"0.2.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.2.1/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.2.1","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.27","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"8bb45416f8ccffc608a4525e5c0d35fb0c1dc9f5","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.2.1.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.3.0":{"name":"event-stream","version":"0.3.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.3.0/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.3.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.27","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"13185462d01da4b299aa15eb69ba58bbbe1d0e4e","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.3.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.4.0":{"name":"event-stream","version":"0.4.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.4.0/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.4.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.27","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"e8c0a3615d4bfb64fb241fbda70aa298c110d78d","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.4.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.5.0":{"name":"event-stream","version":"0.5.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.5.0/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.5.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.27","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"afc04362b4a75ff6693b403b1cc996505f78cd6e","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.5.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.5.1":{"name":"event-stream","version":"0.5.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmJsonOpts":{"file":"/home/dominic/.npm/event-stream/0.5.1/package/package.json","wscript":false,"contributors":false,"serverjs":false},"_id":"event-stream@0.5.1","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.27","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"39b93b81a5c2012e9bf42013248d1fa867ef680b","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.5.1.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"directories":{}},"0.5.2":{"name":"event-stream","version":"0.5.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.5.2","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.101","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"a27dda2021993cfade22037178b6da812dd06ea4","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.5.2.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.5.3":{"name":"event-stream","version":"0.5.3","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.5.3","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.101","_nodeVersion":"v0.4.10","_defaultsLoaded":true,"dist":{"shasum":"b77b9309f7107addfeab63f0c0eafd8db0bd8c1c","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.5.3.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.6.0":{"name":"event-stream","version":"0.6.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.6.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.103","_nodeVersion":"v0.4.12","_defaultsLoaded":true,"dist":{"shasum":"5601c6c61f595010d1eabfdc9265b1ad5e9902b8","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.6.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.7.0":{"name":"event-stream","version":"0.7.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.7.0","engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.0.103","_nodeVersion":"v0.4.12","_defaultsLoaded":true,"dist":{"shasum":"2d0d12412e0a24f56641766932b9558718e0001f","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.7.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.8.0":{"name":"event-stream","version":"0.8.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.8.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"9117185ceba4b8f5cac2e6d8745f011599a3646e","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.8.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.8.1":{"name":"event-stream","version":"0.8.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"1","it-is":"1","d-utils":"2.3"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.8.1","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"7f820fbedd6be767161fbabe725c556e6195f6ef","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.8.1.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.8.2":{"name":"event-stream","version":"0.8.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.8.2","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"f44d557462062ab20ec6198f9bca7193507d7907","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.8.2.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.0":{"name":"event-stream","version":"0.9.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"8d9fbf773800284b48e158320fe84ed172a5ff04","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.1":{"name":"event-stream","version":"0.9.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.1","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"d13bd543138154f503146878ba500c005eb9d5cb","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.1.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.2":{"name":"event-stream","version":"0.9.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.2","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"0768518cf67b8389a07385066578fc1041fd0e84","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.2.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.3":{"name":"event-stream","version":"0.9.3","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.3","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"dda71a9096a86819ca4fd87f0aeaf075cc7162dc","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.3.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.4":{"name":"event-stream","version":"0.9.4","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.4","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"eee7747b2dac5c8eba5e8ccdc2f6576502bd5ee3","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.4.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.6":{"name":"event-stream","version":"0.9.6","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.6","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"b2400917b19c7d56432dab4c9d8bb520492d77a0","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.6.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.7":{"name":"event-stream","version":"0.9.7","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.7","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"c179eaecf314eb43e164da0493628d288e4df879","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.7.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.9.8":{"name":"event-stream","version":"0.9.8","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.9.8","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.10","_defaultsLoaded":true,"dist":{"shasum":"5da9cf3c7900975989db5a68c28e5b3c98ebe03a","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.9.8.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"0.10.0":{"name":"event-stream","version":"0.10.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@0.10.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.17","_defaultsLoaded":true,"dist":{"shasum":"3b6e7dc596371408d0b3286920e32b3f63f35ed5","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-0.10.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"1.0.0":{"name":"event-stream","version":"1.0.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@1.0.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.17","_defaultsLoaded":true,"dist":{"shasum":"d4ab19d0f4bf82174d69384882df1f1752877530","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-1.0.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"1.1.0":{"name":"event-stream","version":"1.1.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@1.1.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.17","_defaultsLoaded":true,"dist":{"shasum":"92b6973d1ce30e6dcf87667b18614926dcaf38f8","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-1.1.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"1.2.0":{"name":"event-stream","version":"1.2.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@1.2.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"e9a3453ece01c90c45cac26616e0d4efa0aff3be","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-1.2.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"1.3.0":{"name":"event-stream","version":"1.3.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@1.3.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"3899301abf82233f69de8e625c0cf530097d130a","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-1.3.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"1.3.1":{"name":"event-stream","version":"1.3.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@1.3.1","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"19d3ae53b92910eb7a3f2bd382a58693b5eb22ab","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-1.3.1.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.0":{"name":"event-stream","version":"2.0.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.0","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"91b5599247c41672a3c75cd86877c2f9589f07f2","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.1":{"name":"event-stream","version":"2.0.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.1","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"817518c25263801c1e2fdc6aac1f231add90801a","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.1.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.2":{"name":"event-stream","version":"2.0.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.2","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"de33221d25115be73893e17679809e1ec1ef0021","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.2.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.3":{"name":"event-stream","version":"2.0.3","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.3","optionalDependencies":{},"engines":{"node":"*"},"_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"15a395ae817ce7e78611f6be00ea9e81ca94ae29","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.3.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.4":{"name":"event-stream","version":"2.0.4","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.4","_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"4ab4245dd021bb58f69a3ef00325598269b15f5d","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.4.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.9":{"name":"event-stream","version":"2.0.9","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.9","_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"38cff1bc33e23d82e2999fd6d840304950fc6a82","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.9.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.0.10":{"name":"event-stream","version":"2.0.10","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.0.10","_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"4be0735cdd4dd1b75e3aa9ab24a6f9e3ce3d3d16","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.0.10.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.0":{"name":"event-stream","version":"2.1.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"_id":"event-stream@2.1.0","_engineSupported":true,"_npmVersion":"1.1.23","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"96d7847097e41920842bcb8a1080399d7dc5c3b6","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.0.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.2":{"name":"event-stream","version":"2.1.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","macgyver":"~1.5"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_id":"event-stream@2.1.2","dist":{"shasum":"e969ae0138e8e8a96a4adb9b9700333481c38ba6","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.2.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.3":{"name":"event-stream","version":"2.1.3","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0","from":"0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"0"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_id":"event-stream@2.1.3","dist":{"shasum":"9192f8fffa44ba199d1122f03f7c59aec1a707e7","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.3.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.4":{"name":"event-stream","version":"2.1.4","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0","from":"0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.1"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_id":"event-stream@2.1.4","dist":{"shasum":"94532198384148ab51bba11d50d66e481c2b3304","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.4.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.5":{"name":"event-stream","version":"2.1.5","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0","from":"0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_id":"event-stream@2.1.5","dist":{"shasum":"84aa89da993870d77240544e1efbe50fd9aa6364","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.5.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.7":{"name":"event-stream","version":"2.1.7","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","from":"~0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_id":"event-stream@2.1.7","dist":{"shasum":"f181e2a82321a6a6ae06e288ba1265070118f7d9","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.7.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.8":{"name":"event-stream","version":"2.1.8","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","from":"~0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"_id":"event-stream@2.1.8","dist":{"shasum":"538423bfa6ed7469340706530f547dea5fb41eb8","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.8.tgz"},"readme":"","maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.1.9":{"name":"event-stream","version":"2.1.9","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","from":"~0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"","_id":"event-stream@2.1.9","dist":{"shasum":"677d8cabd4dae00af0a38d27678f30c2060d2cfa","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.1.9.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.2.0":{"name":"event-stream","version":"2.2.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","from":"~0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"","_id":"event-stream@2.2.0","dist":{"shasum":"be81769056a116a5965fdb09f1b7a39329f30c83","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.2.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.2.1":{"name":"event-stream","version":"2.2.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"0.0.1","from":"~0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## gate (isShut=true) \n\nIf the gate is `shut`, buffer the stream.  \nAll calls to write will return false (pause upstream),  \nand end will not be sent downstream.  \n\nIf the gate is open, let the stream through.  \n\nNamed `shut` instead of close, because close is already kinda meaningful with streams.  \n\nGate is useful for holding off processing a stream until some resource (i.e. a database, or network connection) is ready.  \n\n``` js\n\n  var gate = es.gate()\n  \n  gate.open() //allow the gate to stream\n  \n  gate.close() //buffer the stream, also do not allow 'end' \n\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n## pipeable (streamCreatorFunction,...)\n\nThe arguments to pipable must be functions that return  \ninstances of Stream or async functions.  \n(If a function is returned, it will be turned into a Stream  \nwith `es.map`.)\n\nHere is the first example rewritten to use `pipeable`.\n\n``` js\n//examples/pretty_pipeable.js\nvar inspect = require('util').inspect\n\nif(!module.parent)\n  require('event-stream').pipeable(function () {\n    return function (data, callback) {\n      try {\n        data = JSON.parse(data)\n      } catch (err) {}              //pass non JSON straight through!\n      callback(null, inspect(data))\n      }\n    })  \n  })\n```\n\n``` bash\n\ncurl -sS registry.npmjs.org/event-stream | node pipeable_pretty.js\n\n## or, turn the pipe into a server!\n\nnode pipeable_pretty.js --port 4646\n\ncurl -sS registry.npmjs.org/event-stream | curl -sSNT- localhost:4646\n\n```\n## compatible modules:\n\n  * https://github.com/felixge/node-growing-file  \n    stream changes on file that is being appended to. just like `tail -f`\n\n  * https://github.com/isaacs/sax-js  \n    streaming xml parser\n\n  * https://github.com/mikeal/request  \n    make http requests. request() returns a through stream!\n\n  * https://github.com/TooTallNate/node-throttle  \n    throttle streams on a bytes per second basis (binary streams only, of course)\n    \n  * https://github.com/mikeal/morestreams  \n    buffer input until connected to a pipe.\n    \n  * https://github.com/TooTallNate/node-gzip-stack  \n    compress and decompress raw streams.\n\n  * https://github.com/Floby/node-json-streams  \n    parse json without buffering it first\n    \n  * https://github.com/floby/node-tokenizer  \n    tokenizer\n  \n  * https://github.com/floby/node-parser  \n    general mechanisms for custom parsers\n    \n  * https://github.com/dodo/node-bufferstream  \n    buffer streams until you say (written in C)\n\n  * https://github.com/tim-smart/node-filter  \n    `filter` pipeable string.replace\n    \n\n## almost compatible modules: (1+ these issues)\n\n  * https://github.com/fictorial/json-line-protocol/issues/1  \n    line reader\n    \n  * https://github.com/jahewson/node-byline/issues/1  \n    line reader\n\n  * https://github.com/AvianFlu/ntwitter/issues/3  \n    twitter client\n\n  * https://github.com/swdyh/node-chirpstream/issues/1  \n    twitter client\n    \n  * https://github.com/polotek/evented-twitter/issues/22  \n    twitter client\n\n\n<!--\nTODO, the following methods are not implemented yet.\n\n## sidestream (stream1,...,streamN)\n\nPipes the incoming stream to many writable streams.  \nremits the input stream.\n\n``` js\n  es.sidestream( //will log the stream to a file\n    es.pipeline(\n      es.mapSync(function (j) {return JSON.stringify(j) + '/n'}),\n      fs.createWruteStream(file, {flags: 'a'})\n    )\n```\n\n## merge (stream1,...,streamN)\n\nCreate a readable stream that merges many streams into one.\n\n(Not implemented yet.)\n\n### another pipe example\n\nSEARCH SUBDIRECTORIES FROM CWD  \nFILTER IF NOT A GIT REPO  \nMAP TO GIT STATUS --porclean + the directory  \nFILTER IF EMPTY STATUS  \nprocess.stdout  \nthat will show all the repos which have unstaged changes  \n\n## TODO & applications\n\n  * buffer -- buffer items\n  * rate limiter\n  * save to database\n    * couch\n    * redis\n    * mongo\n    * file(s)\n  * read from database\n    * couch\n    * redis\n    * mongo\n    * file(s)\n  * recursive\n    * search filesystem\n    * scrape web pages (load pages, parse for links, etc)\n    * module dependencies\n  \n-->\n","_id":"event-stream@2.2.1","dist":{"shasum":"43ae270f21a56b9978d0a9018ea878dae3ed36dd","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.2.1.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.2.2":{"name":"event-stream","version":"2.2.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"0.0.1","from":"~0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## gate (isShut=true) \n\nIf the gate is `shut`, buffer the stream.  \nAll calls to write will return false (pause upstream),  \nand end will not be sent downstream.  \n\nIf the gate is open, let the stream through.  \n\nNamed `shut` instead of close, because close is already kinda meaningful with streams.  \n\nGate is useful for holding off processing a stream until some resource (i.e. a database, or network connection) is ready.  \n\n``` js\n\n  var gate = es.gate()\n  \n  gate.open() //allow the gate to stream\n  \n  gate.close() //buffer the stream, also do not allow 'end' \n\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n## pipeable (streamCreatorFunction,...)\n\nThe arguments to pipable must be functions that return  \ninstances of Stream or async functions.  \n(If a function is returned, it will be turned into a Stream  \nwith `es.map`.)\n\nHere is the first example rewritten to use `pipeable`.\n\n``` js\n//examples/pretty_pipeable.js\nvar inspect = require('util').inspect\n\nif(!module.parent)\n  require('event-stream').pipeable(function () {\n    return function (data, callback) {\n      try {\n        data = JSON.parse(data)\n      } catch (err) {}              //pass non JSON straight through!\n      callback(null, inspect(data))\n      }\n    })  \n  })\n```\n\n``` bash\n\ncurl -sS registry.npmjs.org/event-stream | node pipeable_pretty.js\n\n## or, turn the pipe into a server!\n\nnode pipeable_pretty.js --port 4646\n\ncurl -sS registry.npmjs.org/event-stream | curl -sSNT- localhost:4646\n\n```\n## compatible modules:\n\n  * https://github.com/felixge/node-growing-file  \n    stream changes on file that is being appended to. just like `tail -f`\n\n  * https://github.com/isaacs/sax-js  \n    streaming xml parser\n\n  * https://github.com/mikeal/request  \n    make http requests. request() returns a through stream!\n\n  * https://github.com/TooTallNate/node-throttle  \n    throttle streams on a bytes per second basis (binary streams only, of course)\n    \n  * https://github.com/mikeal/morestreams  \n    buffer input until connected to a pipe.\n    \n  * https://github.com/TooTallNate/node-gzip-stack  \n    compress and decompress raw streams.\n\n  * https://github.com/Floby/node-json-streams  \n    parse json without buffering it first\n    \n  * https://github.com/floby/node-tokenizer  \n    tokenizer\n  \n  * https://github.com/floby/node-parser  \n    general mechanisms for custom parsers\n    \n  * https://github.com/dodo/node-bufferstream  \n    buffer streams until you say (written in C)\n\n  * https://github.com/tim-smart/node-filter  \n    `filter` pipeable string.replace\n    \n\n## almost compatible modules: (1+ these issues)\n\n  * https://github.com/fictorial/json-line-protocol/issues/1  \n    line reader\n    \n  * https://github.com/jahewson/node-byline/issues/1  \n    line reader\n\n  * https://github.com/AvianFlu/ntwitter/issues/3  \n    twitter client\n\n  * https://github.com/swdyh/node-chirpstream/issues/1  \n    twitter client\n    \n  * https://github.com/polotek/evented-twitter/issues/22  \n    twitter client\n\n\n<!--\nTODO, the following methods are not implemented yet.\n\n## sidestream (stream1,...,streamN)\n\nPipes the incoming stream to many writable streams.  \nremits the input stream.\n\n``` js\n  es.sidestream( //will log the stream to a file\n    es.pipeline(\n      es.mapSync(function (j) {return JSON.stringify(j) + '/n'}),\n      fs.createWruteStream(file, {flags: 'a'})\n    )\n```\n\n## merge (stream1,...,streamN)\n\nCreate a readable stream that merges many streams into one.\n\n(Not implemented yet.)\n\n### another pipe example\n\nSEARCH SUBDIRECTORIES FROM CWD  \nFILTER IF NOT A GIT REPO  \nMAP TO GIT STATUS --porclean + the directory  \nFILTER IF EMPTY STATUS  \nprocess.stdout  \nthat will show all the repos which have unstaged changes  \n\n## TODO & applications\n\n  * buffer -- buffer items\n  * rate limiter\n  * save to database\n    * couch\n    * redis\n    * mongo\n    * file(s)\n  * read from database\n    * couch\n    * redis\n    * mongo\n    * file(s)\n  * recursive\n    * search filesystem\n    * scrape web pages (load pages, parse for links, etc)\n    * module dependencies\n  \n-->\n","_id":"event-stream@2.2.2","dist":{"shasum":"07c2aadb89f54e24a0cb74714ba89d5fd42e3b58","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.2.2.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"2.2.3":{"name":"event-stream","version":"2.2.3","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"0.0.1","from":"~0","map-stream":"0.0.1"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## gate (isShut=true) \n\nIf the gate is `shut`, buffer the stream.  \nAll calls to write will return false (pause upstream),  \nand end will not be sent downstream.  \n\nIf the gate is open, let the stream through.  \n\nNamed `shut` instead of close, because close is already kinda meaningful with streams.  \n\nGate is useful for holding off processing a stream until some resource (i.e. a database, or network connection) is ready.  \n\n``` js\n\n  var gate = es.gate()\n  \n  gate.open() //allow the gate to stream\n  \n  gate.close() //buffer the stream, also do not allow 'end' \n\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n## pipeable (streamCreatorFunction,...)\n\nThe arguments to pipable must be functions that return  \ninstances of Stream or async functions.  \n(If a function is returned, it will be turned into a Stream  \nwith `es.map`.)\n\nHere is the first example rewritten to use `pipeable`.\n\n``` js\n//examples/pretty_pipeable.js\nvar inspect = require('util').inspect\n\nif(!module.parent)\n  require('event-stream').pipeable(function () {\n    return function (data, callback) {\n      try {\n        data = JSON.parse(data)\n      } catch (err) {}              //pass non JSON straight through!\n      callback(null, inspect(data))\n      }\n    })  \n  })\n```\n\n``` bash\n\ncurl -sS registry.npmjs.org/event-stream | node pipeable_pretty.js\n\n## or, turn the pipe into a server!\n\nnode pipeable_pretty.js --port 4646\n\ncurl -sS registry.npmjs.org/event-stream | curl -sSNT- localhost:4646\n\n```\n## compatible modules:\n\n  * https://github.com/felixge/node-growing-file  \n    stream changes on file that is being appended to. just like `tail -f`\n\n  * https://github.com/isaacs/sax-js  \n    streaming xml parser\n\n  * https://github.com/mikeal/request  \n    make http requests. request() returns a through stream!\n\n  * https://github.com/TooTallNate/node-throttle  \n    throttle streams on a bytes per second basis (binary streams only, of course)\n    \n  * https://github.com/mikeal/morestreams  \n    buffer input until connected to a pipe.\n    \n  * https://github.com/TooTallNate/node-gzip-stack  \n    compress and decompress raw streams.\n\n  * https://github.com/Floby/node-json-streams  \n    parse json without buffering it first\n    \n  * https://github.com/floby/node-tokenizer  \n    tokenizer\n  \n  * https://github.com/floby/node-parser  \n    general mechanisms for custom parsers\n    \n  * https://github.com/dodo/node-bufferstream  \n    buffer streams until you say (written in C)\n\n  * https://github.com/tim-smart/node-filter  \n    `filter` pipeable string.replace\n    \n\n## almost compatible modules: (1+ these issues)\n\n  * https://github.com/fictorial/json-line-protocol/issues/1  \n    line reader\n    \n  * https://github.com/jahewson/node-byline/issues/1  \n    line reader\n\n  * https://github.com/AvianFlu/ntwitter/issues/3  \n    twitter client\n\n  * https://github.com/swdyh/node-chirpstream/issues/1  \n    twitter client\n    \n  * https://github.com/polotek/evented-twitter/issues/22  \n    twitter client\n\n\n<!--\nTODO, the following methods are not implemented yet.\n\n## sidestream (stream1,...,streamN)\n\nPipes the incoming stream to many writable streams.  \nremits the input stream.\n\n``` js\n  es.sidestream( //will log the stream to a file\n    es.pipeline(\n      es.mapSync(function (j) {return JSON.stringify(j) + '/n'}),\n      fs.createWruteStream(file, {flags: 'a'})\n    )\n```\n\n## merge (stream1,...,streamN)\n\nCreate a readable stream that merges many streams into one.\n\n(Not implemented yet.)\n\n### another pipe example\n\nSEARCH SUBDIRECTORIES FROM CWD  \nFILTER IF NOT A GIT REPO  \nMAP TO GIT STATUS --porclean + the directory  \nFILTER IF EMPTY STATUS  \nprocess.stdout  \nthat will show all the repos which have unstaged changes  \n\n## TODO & applications\n\n  * buffer -- buffer items\n  * rate limiter\n  * save to database\n    * couch\n    * redis\n    * mongo\n    * file(s)\n  * read from database\n    * couch\n    * redis\n    * mongo\n    * file(s)\n  * recursive\n    * search filesystem\n    * scrape web pages (load pages, parse for links, etc)\n    * module dependencies\n  \n-->\n","_id":"event-stream@2.2.3","dist":{"shasum":"0250205b259916490f6783b41c2e49b3f01d42f7","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-2.2.3.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.0":{"name":"event-stream","version":"3.0.0","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"0.0.1","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.3"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n## compatible modules:\n\n  * https://github.com/felixge/node-growing-file  \n    stream changes on file that is being appended to. just like `tail -f`\n\n  * https://github.com/isaacs/sax-js  \n    streaming xml parser\n\n  * https://github.com/mikeal/request  \n    make http requests. request() returns a through stream!\n\n  * https://github.com/TooTallNate/node-throttle  \n    throttle streams on a bytes per second basis (binary streams only, of course)\n    \n  * https://github.com/mikeal/morestreams  \n    buffer input until connected to a pipe.\n    \n  * https://github.com/TooTallNate/node-gzip-stack  \n    compress and decompress raw streams.\n\n  * https://github.com/Floby/node-json-streams  \n    parse json without buffering it first\n    \n  * https://github.com/floby/node-tokenizer  \n    tokenizer\n  \n  * https://github.com/floby/node-parser  \n    general mechanisms for custom parsers\n    \n  * https://github.com/dodo/node-bufferstream  \n    buffer streams until you say (written in C)\n\n  * https://github.com/tim-smart/node-filter  \n    `filter` pipeable string.replace\n    \n\n## almost compatible modules: (1+ these issues)\n\n  * https://github.com/fictorial/json-line-protocol/issues/1  \n    line reader\n    \n  * https://github.com/jahewson/node-byline/issues/1  \n    line reader\n\n  * https://github.com/AvianFlu/ntwitter/issues/3  \n    twitter client\n\n  * https://github.com/swdyh/node-chirpstream/issues/1  \n    twitter client\n    \n  * https://github.com/polotek/evented-twitter/issues/22  \n    twitter client\n\n","_id":"event-stream@3.0.0","dist":{"shasum":"5533eb561cff52d9b9673651ba7485841043a9ba","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.0.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.1":{"name":"event-stream","version":"3.0.1","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"0.0.1","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n## compatible modules:\n\n  * https://github.com/felixge/node-growing-file  \n    stream changes on file that is being appended to. just like `tail -f`\n\n  * https://github.com/isaacs/sax-js  \n    streaming xml parser\n\n  * https://github.com/mikeal/request  \n    make http requests. request() returns a through stream!\n\n  * https://github.com/TooTallNate/node-throttle  \n    throttle streams on a bytes per second basis (binary streams only, of course)\n    \n  * https://github.com/mikeal/morestreams  \n    buffer input until connected to a pipe.\n    \n  * https://github.com/TooTallNate/node-gzip-stack  \n    compress and decompress raw streams.\n\n  * https://github.com/Floby/node-json-streams  \n    parse json without buffering it first\n    \n  * https://github.com/floby/node-tokenizer  \n    tokenizer\n  \n  * https://github.com/floby/node-parser  \n    general mechanisms for custom parsers\n    \n  * https://github.com/dodo/node-bufferstream  \n    buffer streams until you say (written in C)\n\n  * https://github.com/tim-smart/node-filter  \n    `filter` pipeable string.replace\n    \n\n## almost compatible modules: (1+ these issues)\n\n  * https://github.com/fictorial/json-line-protocol/issues/1  \n    line reader\n    \n  * https://github.com/jahewson/node-byline/issues/1  \n    line reader\n\n  * https://github.com/AvianFlu/ntwitter/issues/3  \n    twitter client\n\n  * https://github.com/swdyh/node-chirpstream/issues/1  \n    twitter client\n    \n  * https://github.com/polotek/evented-twitter/issues/22  \n    twitter client\n\n","_id":"event-stream@3.0.1","dist":{"shasum":"3aa0bc4562e24ed5aeb6b24e0a95a790a3038ec4","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.1.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.2":{"name":"event-stream","version":"3.0.2","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n## compatible modules:\n\n  * https://github.com/felixge/node-growing-file  \n    stream changes on file that is being appended to. just like `tail -f`\n\n  * https://github.com/isaacs/sax-js  \n    streaming xml parser\n\n  * https://github.com/mikeal/request  \n    make http requests. request() returns a through stream!\n\n  * https://github.com/TooTallNate/node-throttle  \n    throttle streams on a bytes per second basis (binary streams only, of course)\n    \n  * https://github.com/mikeal/morestreams  \n    buffer input until connected to a pipe.\n    \n  * https://github.com/TooTallNate/node-gzip-stack  \n    compress and decompress raw streams.\n\n  * https://github.com/Floby/node-json-streams  \n    parse json without buffering it first\n    \n  * https://github.com/floby/node-tokenizer  \n    tokenizer\n  \n  * https://github.com/floby/node-parser  \n    general mechanisms for custom parsers\n    \n  * https://github.com/dodo/node-bufferstream  \n    buffer streams until you say (written in C)\n\n  * https://github.com/tim-smart/node-filter  \n    `filter` pipeable string.replace\n    \n\n## almost compatible modules: (1+ these issues)\n\n  * https://github.com/fictorial/json-line-protocol/issues/1  \n    line reader\n    \n  * https://github.com/jahewson/node-byline/issues/1  \n    line reader\n\n  * https://github.com/AvianFlu/ntwitter/issues/3  \n    twitter client\n\n  * https://github.com/swdyh/node-chirpstream/issues/1  \n    twitter client\n    \n  * https://github.com/polotek/evented-twitter/issues/22  \n    twitter client\n\n","_id":"event-stream@3.0.2","dist":{"shasum":"fba2eec08a53649049ef99728b15439c9a6f83b3","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.2.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.3":{"name":"event-stream","version":"3.0.3","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"0.0.4","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","_id":"event-stream@3.0.3","dist":{"shasum":"446cce17696c8b1e8af9605c72fa7222aa3a4ec7","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.3.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.4":{"name":"event-stream","version":"3.0.4","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"1.0.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","_id":"event-stream@3.0.4","dist":{"shasum":"a66f568c91ee03bbfe7d42ca5d5905b659b708d6","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.4.tgz"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.5":{"name":"event-stream","version":"3.0.5","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","_id":"event-stream@3.0.5","dist":{"shasum":"a90de982687c9e8b67ed2e599c40943dc8f671e3","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.5.tgz"},"_npmVersion":"1.1.59","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.6":{"name":"event-stream","version":"3.0.6","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","_id":"event-stream@3.0.6","dist":{"shasum":"d26d0198eabb25c474a34c4a5cd5baabbed049b8","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.6.tgz"},"_npmVersion":"1.1.59","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.7":{"name":"event-stream","version":"3.0.7","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"optimist":"0.2","through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","_id":"event-stream@3.0.7","dist":{"shasum":"9e91509d87c286dc247d32ebc77bcbcbd476bcd9","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.7.tgz"},"_npmVersion":"1.1.59","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.8":{"name":"event-stream","version":"3.0.8","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a simgle 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.8","dist":{"shasum":"37ab05d38b963623ee176e48787259e2ccd2ac04","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.8.tgz"},"_npmVersion":"1.1.65","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.9":{"name":"event-stream","version":"3.0.9","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.9","dist":{"shasum":"db886a8724f6fec30e5e468befa15e87316b800f","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.9.tgz"},"_npmVersion":"1.1.65","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.10":{"name":"event-stream","version":"3.0.10","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.0.0","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2"},"scripts":{"test":"asynct test/"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.10","dist":{"shasum":"6d0604708a63143be03b490f00d7010cac020c2e","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.10.tgz"},"_npmVersion":"1.1.65","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.11":{"name":"event-stream","version":"3.0.11","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.1","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2","tape":"~0.1.5"},"scripts":{"test":"asynct test/","test_tap":"set -e; for t in test/*.js; do node $t; done"},"testling":{"files":"test/*.js","browsers":{"ie":[8,9],"firefox":[13],"chrome":[20],"safari":[5.1],"opera":[12]}},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (seperator)\n\ncreate a through stream that emits `seperator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline seperated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.11","dist":{"shasum":"bc1e340fd94903903822b1a90f14d9fdcaa6333f","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.11.tgz"},"_npmVersion":"1.1.69","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.12":{"name":"event-stream","version":"3.0.12","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.2","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2","tape":"~0.1.5"},"scripts":{"test":"asynct test/","test_tap":"set -e; for t in test/*.js; do node $t; done"},"testling":{"files":"test/*.js","browsers":{"ie":[8,9],"firefox":[13],"chrome":[20],"safari":[5.1],"opera":[12]}},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/streams.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (separator)\n\ncreate a through stream that emits `separator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline separated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.12","dist":{"shasum":"1c85e3a219663ad7876d9e8bdb6a2bc187b040ba","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.12.tgz"},"_from":".","_npmVersion":"1.2.3","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.13":{"name":"event-stream","version":"3.0.13","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"1.1.0","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.1","pause-stream":"0.0.4","split":"0.2","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2","tape":"~0.1.5"},"scripts":{"test":"asynct test/","test_tap":"set -e; for t in test/*.js; do node $t; done"},"testling":{"files":"test/*.js","browsers":{"ie":[8,9],"firefox":[13],"chrome":[20],"safari":[5.1],"opera":[12]}},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"optionalDependencies":{},"engines":{"node":"*"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/strems.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (separator)\n\ncreate a through stream that emits `separator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline separated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.13","dist":{"shasum":"33bef11a4a2f5df96ab5b52680f3c7e2c073ea02","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.13.tgz"},"_from":".","_npmVersion":"1.2.3","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.14":{"name":"event-stream","version":"3.0.14","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"~2.3.1","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.2","pause-stream":"0.0.10","split":"0.2","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2","tape":"~0.1.5"},"scripts":{"test":"asynct test/","test_tap":"set -e; for t in test/*.js; do node $t; done"},"testling":{"files":"test/*.js","browsers":{"ie":[8,9],"firefox":[13],"chrome":[20],"safari":[5.1],"opera":[12]}},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/strems.html \"Stream\") are nodes best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (separator)\n\ncreate a through stream that emits `separator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline separated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","_id":"event-stream@3.0.14","dist":{"shasum":"786d78b53b3019e30aeabc72829ff9cf32308a26","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.14.tgz"},"_from":".","_npmVersion":"1.2.3","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.15":{"name":"event-stream","version":"3.0.15","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"~2.3.1","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.2","pause-stream":"0.0.10","split":"0.2","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2","tape":"~0.1.5"},"scripts":{"test":"asynct test/","test_tap":"set -e; for t in test/*.js; do node $t; done"},"testling":{"files":"test/*.js","browsers":{"ie":[8,9],"firefox":[13],"chrome":[20],"safari":[5.1],"opera":[12]}},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/strems.html \"Stream\") are node's best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\nStream API docs: [nodejs.org/api/streams](http://nodejs.org/api/streams.html \"Stream\")\n\nNOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/streams.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (separator)\n\ncreate a through stream that emits `separator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline separated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","bugs":{"url":"https://github.com/dominictarr/event-stream/issues"},"_id":"event-stream@3.0.15","dist":{"shasum":"82bfd046fa465583a76bb850e64e15da853e8c5d","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.15.tgz"},"_from":".","_npmVersion":"1.2.30","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}},"3.0.16":{"name":"event-stream","version":"3.0.16","description":"construct pipes of streams of events","homepage":"http://github.com/dominictarr/event-stream","repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"dependencies":{"through":"~2.3.1","duplexer":"~0.0.2","from":"~0","map-stream":"0.0.2","pause-stream":"0.0.10","split":"0.2","stream-combiner":"0.0.0"},"devDependencies":{"asynct":"*","it-is":"1","ubelt":"~2.9","stream-spec":"~0.2","tape":"~0.1.5"},"scripts":{"test":"asynct test/","test_tap":"set -e; for t in test/*.js; do node $t; done"},"testling":{"files":"test/*.js","browsers":{"ie":[8,9],"firefox":[13],"chrome":[20],"safari":[5.1],"opera":[12]}},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"readme":"# EventStream\n\n<img src=https://secure.travis-ci.org/dominictarr/event-stream.png?branch=master>\n\n[![browser status](http://ci.testling.com/dominictarr/event-stream.png)]\n(http://ci.testling.com/dominictarr/event-stream)\n\n[Streams](http://nodejs.org/api/strems.html \"Stream\") are node's best and most misunderstood idea, and \n_<em>EventStream</em>_ is a toolkit to make creating and working with streams <em>easy</em>.  \n\nNormally, streams are only used of IO,  \nbut in event stream we send all kinds of objects down the pipe.  \nIf your application's <em>input</em> and <em>output</em> are streams,  \nshouldn't the <em>throughput</em> be a stream too?  \n\nThe *EventStream* functions resemble the array functions,  \nbecause Streams are like Arrays, but laid out in time, rather than in memory.  \n\n<em>All the `event-stream` functions return instances of `Stream`</em>.\n\n`event-stream` creates \n[0.8 streams](https://github.com/joyent/node/blob/v0.8/doc/api/stream.markdown)\n, which are compatible with [0.10 streams](http://nodejs.org/api/stream.html \"Stream\")\n\n>NOTE: I shall use the term <em>\"through stream\"</em> to refer to a stream that is writable <em>and</em> readable.  \n\n###[simple example](https://github.com/dominictarr/event-stream/blob/master/examples/pretty.js):\n\n``` js\n\n//pretty.js\n\nif(!module.parent) {\n  var es = require('event-stream')\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n  }\n```\nrun it ...\n\n``` bash  \ncurl -sS registry.npmjs.org/event-stream | node pretty.js\n```\n \n[node Stream documentation](http://nodejs.org/api/stream.html)\n\n## through (write?, end?)\n\nReemits data synchronously. Easy way to create syncronous through streams.\nPass in an optional `write` and `end` methods. They will be called in the \ncontext of the stream. Use `this.pause()` and `this.resume()` to manage flow.\nCheck `this.paused` to see current flow state. (write always returns `!this.paused`)\n\nthis function is the basis for most of the syncronous streams in `event-stream`.\n\n``` js\n\nes.through(function write(data) {\n    this.emit('data', data)\n    //this.pause() \n  },\n  function end () { //optional\n    this.emit('end')\n  })\n\n```\n\n##map (asyncFunction)\n\nCreate a through stream from an asyncronous function.  \n\n``` js\nvar es = require('event-stream')\n\nes.map(function (data, callback) {\n  //transform data\n  // ...\n  callback(null, data)\n})\n\n```\n\nEach map MUST call the callback. It may callback with data, with an error or with no arguments, \n\n  * `callback()` drop this data.  \n    this makes the map work like `filter`,  \n    note:`callback(null,null)` is not the same, and will emit `null`\n\n  * `callback(null, newData)` turn data into newData\n    \n  * `callback(error)` emit an error for this item.\n\n>Note: if a callback is not called, `map` will think that it is still being processed,   \n>every call must be answered or the stream will not know when to end.  \n>\n>Also, if the callback is called more than once, every call but the first will be ignored.\n\n## mapSync (syncFunction)\n\nSame as `map`, but the callback is called synchronously. Based on `es.through`\n\n## split (matcher)\n\nBreak up a stream and reassemble it so that each line is a chunk. matcher may be a `String`, or a `RegExp` \n\nExample, read every line in a file ...\n\n``` js\n  es.pipeline(\n    fs.createReadStream(file, {flags: 'r'}),\n    es.split(),\n    es.map(function (line, cb) {\n       //do something with the line \n       cb(null, line)\n    })\n  )\n\n```\n\n`split` takes the same arguments as `string.split` except it defaults to '\\n' instead of ',', and the optional `limit` paremeter is ignored.\n[String#split](https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/String/split)\n\n## join (separator)\n\ncreate a through stream that emits `separator` between each chunk, just like Array#join.\n\n(for legacy reasons, if you pass a callback instead of a string, join is a synonym for `es.wait`)\n\n## replace (from, to)\n\nReplace all occurences of `from` with `to`. `from` may be a `String` or a `RegExp`.  \nWorks just like `string.split(from).join(to)`, but streaming.\n\n\n## parse\n\nConvienience function for parsing JSON chunks. For newline separated JSON,\nuse with `es.split`\n\n``` js\nfs.createReadStream(filename)\n  .pipe(es.split()) //defaults to lines.\n  .pipe(es.parse())\n```\n\n## stringify\n\nconvert javascript objects into lines of text. The text will have whitespace escaped and have a `\\n` appended, so it will be compatible with `es.parse`\n\n``` js\nobjectStream\n  .pipe(es.stringify())\n  .pipe(fs.createWriteStream(filename))\n```\n\n##readable (asyncFunction) \n\ncreate a readable stream (that respects pause) from an async function.  \nwhile the stream is not paused,  \nthe function will be polled with `(count, callback)`,  \nand `this`  will be the readable stream.\n\n``` js\n\nes.readable(function (count, callback) {\n  if(streamHasEnded)\n    return this.emit('end')\n  \n  //...\n  \n  this.emit('data', data) //use this way to emit multiple chunks per call.\n      \n  callback() // you MUST always call the callback eventually.\n             // the function will not be called again until you do this.\n})\n```\nyou can also pass the data and the error to the callback.  \nyou may only call the callback once.  \ncalling the same callback more than once will have no effect.  \n\n##readArray (array)\n\nCreate a readable stream from an Array.\n\nJust emit each item as a data event, respecting `pause` and `resume`.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1,2,3])\n\n  reader.pipe(...)\n```\n\n## writeArray (callback)\n\ncreate a writeable stream from a callback,  \nall `data` events are stored in an array, which is passed to the callback when the stream ends.\n\n``` js\n  var es = require('event-stream')\n    , reader = es.readArray([1, 2, 3])\n    , writer = es.writeArray(function (err, array){\n      //array deepEqual [1, 2, 3]\n    })\n\n  reader.pipe(writer)\n```\n\n## pipeline (stream1,...,streamN)\n\nTurn a pipeline into a single stream. `pipeline` returns a stream that writes to the first stream\nand reads from the last stream. \n\nListening for 'error' will recieve errors from all streams inside the pipe.\n\n> `connect` is an alias for `pipeline`.\n\n``` js\n\n  es.pipeline(                         //connect streams together with `pipe`\n    process.openStdin(),              //open stdin\n    es.split(),                       //split stream to break on newlines\n    es.map(function (data, callback) {//turn this async function into a stream\n      callback(null\n        , inspect(JSON.parse(data)))  //render it nicely\n    }),\n    process.stdout                    // pipe it to stdout !\n    )\n```\n\n## pause  () \n\nA stream that buffers all chunks when paused.\n\n\n``` js\n  var ps = es.pause()\n  ps.pause() //buffer the stream, also do not allow 'end' \n  ps.resume() //allow chunks through\n```\n\n## duplex (writeStream, readStream)\n\nTakes a writable stream and a readable stream and makes them appear as a readable writable stream.\n\nIt is assumed that the two streams are connected to each other in some way.  \n\n(This is used by `pipeline` and `child`.)\n\n``` js\n  var grep = cp.exec('grep Stream')\n\n  es.duplex(grep.stdin, grep.stdout)\n```\n\n## child (child_process)\n\nCreate a through stream from a child process ...\n\n``` js\n  var cp = require('child_process')\n\n  es.child(cp.exec('grep Stream')) // a through stream\n\n```\n\n## wait (callback)\n\nwaits for stream to emit 'end'.\njoins chunks of a stream into a single string. \ntakes an optional callback, which will be passed the \ncomplete string when it receives the 'end' event.\n\nalso, emits a single 'data' event.\n\n``` js\n\nreadStream.pipe(es.join(function (err, text) {\n  // have complete text here.\n}))\n\n```\n\n\n","readmeFilename":"readme.markdown","bugs":{"url":"https://github.com/dominictarr/event-stream/issues"},"_id":"event-stream@3.0.16","dist":{"shasum":"3523458eb1d827fac6ae26a7563dbae9c0e9a43b","tarball":"http://registry.npmjs.org/event-stream/-/event-stream-3.0.16.tgz"},"_from":".","_npmVersion":"1.3.0","_npmUser":{"name":"dominictarr","email":"dominic.tarr@gmail.com"},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"directories":{}}},"maintainers":[{"name":"dominictarr","email":"dominic.tarr@gmail.com"}],"time":{"0.1.0":"2011-12-07T06:11:27.548Z","0.2.0":"2011-12-07T06:11:27.548Z","0.2.1":"2011-12-07T06:11:27.548Z","0.3.0":"2011-12-07T06:11:27.548Z","0.4.0":"2011-12-07T06:11:27.548Z","0.5.0":"2011-12-07T06:11:27.548Z","0.5.1":"2011-12-07T06:11:27.548Z","0.5.2":"2011-11-01T00:33:33.526Z","0.5.3":"2011-11-01T01:16:47.365Z","0.6.0":"2011-11-07T08:42:04.761Z","0.7.0":"2011-12-07T06:11:27.548Z","0.8.0":"2012-03-25T22:39:41.344Z","0.8.1":"2012-04-07T00:59:16.484Z","0.8.2":"2012-04-07T03:32:02.580Z","0.9.0":"2012-04-21T08:16:38.740Z","0.9.1":"2012-04-21T08:26:32.603Z","0.9.2":"2012-04-21T09:44:59.748Z","0.9.3":"2012-04-25T07:46:19.655Z","0.9.4":"2012-04-25T08:49:58.980Z","0.9.6":"2012-04-25T09:18:33.569Z","0.9.7":"2012-04-25T09:29:15.354Z","0.9.8":"2012-05-03T02:47:33.747Z","0.10.0":"2012-05-17T14:57:58.798Z","1.0.0":"2012-05-21T05:28:09.492Z","1.1.0":"2012-05-21T06:05:51.172Z","1.2.0":"2012-05-25T18:21:30.518Z","1.3.0":"2012-05-27T16:43:09.641Z","1.3.1":"2012-06-05T07:15:53.054Z","2.0.0":"2012-06-05T14:57:16.522Z","2.0.1":"2012-06-12T16:21:21.947Z","2.0.2":"2012-06-12T16:24:36.486Z","2.0.3":"2012-06-12T16:36:15.993Z","2.0.4":"2012-06-14T14:20:41.712Z","2.0.9":"2012-06-26T09:19:26.919Z","2.0.10":"2012-06-26T10:47:58.161Z","2.1.0":"2012-06-26T12:26:12.837Z","2.1.2":"2012-06-30T12:40:11.854Z","2.1.3":"2012-07-04T16:31:17.609Z","2.1.4":"2012-07-04T17:12:30.551Z","2.1.5":"2012-07-11T12:05:48.438Z","2.1.7":"2012-07-19T16:52:10.426Z","2.1.8":"2012-07-23T20:34:31.619Z","2.1.9":"2012-08-04T12:59:26.323Z","2.2.0":"2012-08-07T14:52:31.032Z","2.2.1":"2012-08-10T12:31:18.687Z","2.2.2":"2012-08-15T12:26:09.720Z","2.2.3":"2012-08-18T13:10:53.974Z","3.0.0":"2012-08-18T13:39:02.524Z","3.0.1":"2012-08-18T14:18:00.038Z","3.0.2":"2012-08-19T19:44:58.461Z","3.0.3":"2012-08-30T14:25:05.439Z","3.0.4":"2012-08-31T09:40:06.165Z","3.0.5":"2012-09-25T23:02:28.550Z","3.0.6":"2012-09-30T22:39:48.407Z","3.0.7":"2012-09-30T22:43:59.240Z","3.0.8":"2012-11-27T07:27:28.661Z","3.0.9":"2012-12-12T04:48:07.024Z","3.0.10":"2012-12-13T00:57:45.839Z","3.0.11":"2013-01-06T10:32:41.136Z","3.0.12":"2013-03-09T00:18:56.430Z","3.0.13":"2013-03-27T10:03:30.034Z","3.0.14":"2013-04-22T23:37:56.657Z","3.0.15":"2013-06-18T15:25:59.980Z","3.0.16":"2013-07-13T23:10:11.738Z"},"author":{"name":"Dominic Tarr","email":"dominic.tarr@gmail.com","url":"http://bit.ly/dominictarr"},"repository":{"type":"git","url":"git://github.com/dominictarr/event-stream.git"},"users":{"substack":true,"fgribreau":true,"xenomuta":true,"jswartwood":true,"carlos8f":true,"hughsk":true,"michaelnisi":true,"hij1nx":true,"dubban":true},"_attachments":{"event-stream-3.0.16.tgz":{"content_type":"application/octet-stream","revpos":141,"digest":"md5-wFB/WeNO4Kr3xV6LX02ljA==","length":12376,"stub":true},"event-stream-3.0.15.tgz":{"content_type":"application/octet-stream","revpos":138,"digest":"md5-m1Dz7qZoTZ33B51V0AxlYA==","length":12333,"stub":true},"event-stream-3.0.14.tgz":{"content_type":"application/octet-stream","revpos":136,"digest":"md5-7fPLbyJo9NxxWfp+Zc7bZQ==","length":12311,"stub":true},"event-stream-3.0.13.tgz":{"content_type":"application/octet-stream","revpos":134,"digest":"md5-o1Tqlgrmdyk5VTrscU/1bA==","length":12316,"stub":true},"event-stream-3.0.12.tgz":{"content_type":"application/octet-stream","revpos":131,"digest":"md5-call7qNL4G7+bfiJFh7v9g==","length":12311,"stub":true},"event-stream-3.0.11.tgz":{"content_type":"application/octet-stream","revpos":128,"digest":"md5-/3La7EVD9NzgJrpU6RN3qQ==","length":12305,"stub":true},"event-stream-3.0.10.tgz":{"content_type":"application/octet-stream","revpos":125,"digest":"md5-iWq00bgZLEQHocCDQnNhVg==","length":12089,"stub":true},"event-stream-3.0.9.tgz":{"content_type":"application/octet-stream","revpos":123,"digest":"md5-i+wBgw3GaA0+kA9l5KpLLQ==","length":12059,"stub":true},"event-stream-3.0.8.tgz":{"content_type":"application/octet-stream","revpos":121,"digest":"md5-sBoR2EmzkqnGrffqjCVcxg==","length":12058,"stub":true},"event-stream-3.0.7.tgz":{"content_type":"application/octet-stream","revpos":119,"digest":"md5-elVU1JQqh57AJvb746p3sQ==","length":12356,"stub":true},"event-stream-3.0.6.tgz":{"content_type":"application/octet-stream","revpos":117,"digest":"md5-VyqqiaZheP+WEYid0EpKnw==","length":12694,"stub":true},"event-stream-3.0.5.tgz":{"content_type":"application/octet-stream","revpos":115,"digest":"md5-S6oDxnGVJ7/+dy6i08nYmQ==","length":12460,"stub":true},"event-stream-3.0.4.tgz":{"content_type":"application/octet-stream","revpos":111,"digest":"md5-QQF5StwOvT6pNaTov9VVKQ==","length":12454,"stub":true},"event-stream-3.0.3.tgz":{"content_type":"application/octet-stream","revpos":109,"digest":"md5-k3NY9bYK2yOUfcSsdCKBxw==","length":12445,"stub":true},"event-stream-3.0.2.tgz":{"content_type":"application/octet-stream","revpos":106,"digest":"md5-nh0o8e+a6DsBXvb+9SL4vQ==","length":13033,"stub":true},"event-stream-3.0.1.tgz":{"content_type":"application/octet-stream","revpos":104,"digest":"md5-9yWXe0dBHy5fF4MI1t7/dA==","length":13029,"stub":true},"event-stream-3.0.0.tgz":{"content_type":"application/octet-stream","revpos":102,"digest":"md5-7+8aP0f/g2Pu86piBib3IQ==","length":13037,"stub":true},"event-stream-2.2.3.tgz":{"content_type":"application/octet-stream","revpos":100,"digest":"md5-lwc/BRr95v/b11B0oEVDtA==","length":14739,"stub":true},"event-stream-2.2.2.tgz":{"content_type":"application/octet-stream","revpos":98,"digest":"md5-Qu3QbX8YxtSK9o1z08XbFQ==","length":15220,"stub":true},"event-stream-2.2.1.tgz":{"content_type":"application/octet-stream","revpos":94,"digest":"md5-sMDWmcub3TFvVeD4nlc3fg==","length":15632,"stub":true},"event-stream-2.2.0.tgz":{"content_type":"application/octet-stream","revpos":92,"digest":"md5-HGCemgZQT2xhj2mC6k6FBw==","length":15611,"stub":true},"event-stream-2.1.9.tgz":{"content_type":"application/octet-stream","revpos":90,"digest":"md5-vwSEVDt2SoL3uErT+5EJNQ==","length":15488,"stub":true},"event-stream-2.1.8.tgz":{"content_type":"application/octet-stream","revpos":86,"digest":"md5-cnzV9pHTB8gfSHePZKGNYg==","length":15397,"stub":true},"event-stream-2.1.7.tgz":{"content_type":"application/octet-stream","revpos":84,"digest":"md5-iucx9ie910S5wGLXzoG0qg==","length":15368,"stub":true},"event-stream-2.1.5.tgz":{"content_type":"application/octet-stream","revpos":82,"digest":"md5-Rrza9KLyVW0AFiTNPgpAfw==","length":15182,"stub":true},"event-stream-2.1.4.tgz":{"content_type":"application/octet-stream","revpos":80,"digest":"md5-0Oaxao3JZ7ulKlGyW0Ujwg==","length":15242,"stub":true},"event-stream-2.1.3.tgz":{"content_type":"application/octet-stream","revpos":78,"digest":"md5-WsQQs1VG6wWP8rzke+k47Q==","length":15236,"stub":true},"event-stream-2.1.2.tgz":{"content_type":"application/octet-stream","revpos":76,"digest":"md5-c0CSgvnmabzQ9cZpJayZMA==","length":15364,"stub":true},"event-stream-2.1.0.tgz":{"content_type":"application/octet-stream","revpos":74,"digest":"md5-+lAWk8DEB14S5lKXKQBExw==","length":14513,"stub":true},"event-stream-2.0.10.tgz":{"content_type":"application/octet-stream","revpos":72,"digest":"md5-FaAR+1NEpEYvDsHJIVAzoA==","length":14462,"stub":true},"event-stream-2.0.9.tgz":{"content_type":"application/octet-stream","revpos":70,"digest":"md5-0pgAoohx/H0bQ22XiEL51A==","length":14450,"stub":true},"event-stream-2.0.4.tgz":{"content_type":"application/octet-stream","revpos":68,"digest":"md5-/aI4kFVsTugU+Rwz9euD8w==","length":14363,"stub":true},"event-stream-2.0.3.tgz":{"content_type":"application/octet-stream","revpos":66,"digest":"md5-XGSObiNhtAKag7KITae5sw==","length":14184,"stub":true},"event-stream-2.0.2.tgz":{"content_type":"application/octet-stream","revpos":64,"digest":"md5-8gk4Miv6jrzkeYOnviK3Aw==","length":14172,"stub":true},"event-stream-2.0.1.tgz":{"content_type":"application/octet-stream","revpos":62,"digest":"md5-pfW2aNyrV8HqdI2hdMENUw==","length":14172,"stub":true},"event-stream-2.0.0.tgz":{"content_type":"application/octet-stream","revpos":60,"digest":"md5-y7xInAFwAS2gG4Gbf1bnfg==","length":14143,"stub":true},"event-stream-1.3.1.tgz":{"content_type":"application/octet-stream","revpos":58,"digest":"md5-1Cz2pCQgtYZBIbwN1TDnIw==","length":13944,"stub":true},"event-stream-1.3.0.tgz":{"content_type":"application/octet-stream","revpos":56,"digest":"md5-jBcjHZU6GrIvfJ4n1IVL8w==","length":13830,"stub":true},"event-stream-1.2.0.tgz":{"content_type":"application/octet-stream","revpos":54,"digest":"md5-DcBHdswDwQDcWp/1k6zWMQ==","length":13605,"stub":true},"event-stream-1.1.0.tgz":{"content_type":"application/octet-stream","revpos":52,"digest":"md5-oFmU/j/fDox2dFB47/sklA==","length":13544,"stub":true},"event-stream-1.0.0.tgz":{"content_type":"application/octet-stream","revpos":50,"digest":"md5-zecFR8Fg1AHgVLN8iQFOlQ==","length":13229,"stub":true},"event-stream-0.10.0.tgz":{"content_type":"application/octet-stream","revpos":48,"digest":"md5-qO9dBg5iXCQv3/grCuGhhg==","length":13243,"stub":true},"event-stream-0.9.8.tgz":{"content_type":"application/octet-stream","revpos":46,"digest":"md5-KLNgjBdv2KszDktJwaTd8Q==","length":12952,"stub":true},"event-stream-0.9.7.tgz":{"content_type":"application/octet-stream","revpos":44,"digest":"md5-pYuafmiwo5kfaU8hIk53ng==","length":12684,"stub":true},"event-stream-0.9.6.tgz":{"content_type":"application/octet-stream","revpos":42,"digest":"md5-NZgmqDFrK9gbB/uP8R+STQ==","length":12678,"stub":true},"event-stream-0.9.4.tgz":{"content_type":"application/octet-stream","revpos":40,"digest":"md5-JTsVW+7u6J37yZdfHk/zXg==","length":12698,"stub":true},"event-stream-0.9.3.tgz":{"content_type":"application/octet-stream","revpos":38,"digest":"md5-04aZqYu7eVMe5o9kF3advA==","length":12702,"stub":true},"event-stream-0.9.2.tgz":{"content_type":"application/octet-stream","revpos":36,"digest":"md5-GYGtSzgxkqyKJwR537/TTQ==","length":12623,"stub":true},"event-stream-0.9.1.tgz":{"content_type":"application/octet-stream","revpos":34,"digest":"md5-A1RawAr86pQSrV8NRQ8NxA==","length":12617,"stub":true},"event-stream-0.9.0.tgz":{"content_type":"application/octet-stream","revpos":32,"digest":"md5-3i9eOd5ke1hzWMW/1LpCsQ==","length":12652,"stub":true},"event-stream-0.8.2.tgz":{"content_type":"application/octet-stream","revpos":30,"digest":"md5-loHr7ut4GPAnfbA+lN96Gw==","length":12442,"stub":true},"event-stream-0.8.1.tgz":{"content_type":"application/octet-stream","revpos":28,"digest":"md5-kRleVpBbh9jtk68fFhCIbQ==","length":8466,"stub":true},"event-stream-0.8.0.tgz":{"content_type":"application/octet-stream","revpos":26,"digest":"md5-n4/XzVfUQmj1a95UiCM+yQ==","length":8566,"stub":true},"event-stream-0.7.0.tgz":{"content_type":"application/octet-stream","revpos":24,"digest":"md5-pl0AZtVjoMCQ3Gj6edNTUQ==","length":10111,"stub":true},"event-stream-0.6.0.tgz":{"content_type":"application/octet-stream","revpos":21,"digest":"md5-/ax+epqrAJny5A/wek8aUQ==","length":10022,"stub":true},"event-stream-0.5.3.tgz":{"content_type":"application/octet-stream","revpos":19,"digest":"md5-QNDkqI+s84cg1WMZLYok2w==","length":9785,"stub":true},"event-stream-0.5.2.tgz":{"content_type":"application/octet-stream","revpos":17,"digest":"md5-Vs+xePA1NTZ0zMIh9EDn6w==","length":9752,"stub":true},"event-stream-0.5.1.tgz":{"content_type":"application/octet-stream","revpos":15,"digest":"md5-GLRBWTvAmakU2SgfFGM9pg==","length":9002,"stub":true},"event-stream-0.5.0.tgz":{"content_type":"application/octet-stream","revpos":13,"digest":"md5-gJjxSqitAlRB7NPIoLbZFw==","length":8696,"stub":true},"event-stream-0.4.0.tgz":{"content_type":"application/octet-stream","revpos":11,"digest":"md5-2pi5Ds6M8TwLSbdfUEaF9A==","length":8477,"stub":true},"event-stream-0.3.0.tgz":{"content_type":"application/octet-stream","revpos":9,"digest":"md5-LOTQEtyklqwg4OIc1qTEiw==","length":8425,"stub":true},"event-stream-0.2.1.tgz":{"content_type":"application/octet-stream","revpos":7,"digest":"md5-/LdjoBLOtQiQGM27Wf1Mmw==","length":7903,"stub":true},"event-stream-0.2.0.tgz":{"content_type":"application/octet-stream","revpos":5,"digest":"md5-DvQI1uqb55PMzRutgoM5eQ==","length":5411,"stub":true},"event-stream-0.1.0.tgz":{"content_type":"application/octet-stream","revpos":3,"digest":"md5-rne0UaNZUJUejmsOEFU8ww==","length":2342,"stub":true}},"_etag":"\"BVYRMQHPNTNT171SO967S72V6\""}